{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from model import AlexNet\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='../data/CIFAR10/'\n",
    "    ,train=True\n",
    "    ,download=False\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize((224, 224))\n",
    "        \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.303879499435425\n",
      "loss2: 2.2895069122314453\n"
     ]
    }
   ],
   "source": [
    "network = AlexNet(num_classes=10, init_weights=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size = 100)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.0001)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images,labels = batch\n",
    "\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "\n",
    "loss.backward()#计算梯度\n",
    "optimizer.step()#更新权重\n",
    "\n",
    "#-----------------------\n",
    "print('loss1:',loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "print('loss2:',loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where 0 loss: 2.302605628967285\n",
      "where 1 loss: 2.3044912815093994\n",
      "where 2 loss: 2.295905590057373\n",
      "where 3 loss: 2.2903661727905273\n",
      "where 4 loss: 2.313140392303467\n",
      "where 5 loss: 2.2995657920837402\n",
      "where 6 loss: 2.3074772357940674\n",
      "where 7 loss: 2.3095266819000244\n",
      "where 8 loss: 2.2940754890441895\n",
      "where 9 loss: 2.2939670085906982\n",
      "where 10 loss: 2.287233829498291\n",
      "where 11 loss: 2.30163836479187\n",
      "where 12 loss: 2.2912256717681885\n",
      "where 13 loss: 2.2848684787750244\n",
      "where 14 loss: 2.2791860103607178\n",
      "where 15 loss: 2.2820322513580322\n",
      "where 16 loss: 2.2649595737457275\n",
      "where 17 loss: 2.2681684494018555\n",
      "where 18 loss: 2.3001832962036133\n",
      "where 19 loss: 2.2688422203063965\n",
      "where 20 loss: 2.272469997406006\n",
      "where 21 loss: 2.2145180702209473\n",
      "where 22 loss: 2.2173807621002197\n",
      "where 23 loss: 2.1845390796661377\n",
      "where 24 loss: 2.112027645111084\n",
      "where 25 loss: 2.1308460235595703\n",
      "where 26 loss: 2.155186653137207\n",
      "where 27 loss: 2.1003923416137695\n",
      "where 28 loss: 2.0930821895599365\n",
      "where 29 loss: 2.172036647796631\n",
      "where 30 loss: 2.1146559715270996\n",
      "where 31 loss: 2.218907356262207\n",
      "where 32 loss: 2.05985951423645\n",
      "where 33 loss: 2.0451459884643555\n",
      "where 34 loss: 2.0663809776306152\n",
      "where 35 loss: 2.0207407474517822\n",
      "where 36 loss: 2.108295440673828\n",
      "where 37 loss: 1.9372689723968506\n",
      "where 38 loss: 2.0321390628814697\n",
      "where 39 loss: 2.152390956878662\n",
      "where 40 loss: 1.9926390647888184\n",
      "where 41 loss: 1.9932407140731812\n",
      "where 42 loss: 1.9670490026474\n",
      "where 43 loss: 2.008474349975586\n",
      "where 44 loss: 2.0869414806365967\n",
      "where 45 loss: 2.0173511505126953\n",
      "where 46 loss: 2.000422239303589\n",
      "where 47 loss: 1.9149293899536133\n",
      "where 48 loss: 2.043118953704834\n",
      "where 49 loss: 1.8363643884658813\n",
      "where 50 loss: 1.9459822177886963\n",
      "where 51 loss: 2.109910011291504\n",
      "where 52 loss: 2.1071245670318604\n",
      "where 53 loss: 1.7584925889968872\n",
      "where 54 loss: 2.046286106109619\n",
      "where 55 loss: 1.9307570457458496\n",
      "where 56 loss: 1.8471275568008423\n",
      "where 57 loss: 1.983930230140686\n",
      "where 58 loss: 1.8447459936141968\n",
      "where 59 loss: 1.9456629753112793\n",
      "where 60 loss: 1.8048768043518066\n",
      "where 61 loss: 1.8880795240402222\n",
      "where 62 loss: 1.9529951810836792\n",
      "where 63 loss: 1.8596887588500977\n",
      "where 64 loss: 1.9286370277404785\n",
      "where 65 loss: 1.8450877666473389\n",
      "where 66 loss: 1.9699273109436035\n",
      "where 67 loss: 1.743870735168457\n",
      "where 68 loss: 1.9306244850158691\n",
      "where 69 loss: 1.8682729005813599\n",
      "where 70 loss: 1.8590219020843506\n",
      "where 71 loss: 1.9097764492034912\n",
      "where 72 loss: 1.7218918800354004\n",
      "where 73 loss: 1.8554580211639404\n",
      "where 74 loss: 1.8207660913467407\n",
      "where 75 loss: 1.8548109531402588\n",
      "where 76 loss: 1.9950833320617676\n",
      "where 77 loss: 1.9123337268829346\n",
      "where 78 loss: 1.8907071352005005\n",
      "where 79 loss: 1.757685899734497\n",
      "where 80 loss: 1.8935546875\n",
      "where 81 loss: 1.8474351167678833\n",
      "where 82 loss: 1.8091322183609009\n",
      "where 83 loss: 1.9782108068466187\n",
      "where 84 loss: 1.8338134288787842\n",
      "where 85 loss: 1.7191767692565918\n",
      "where 86 loss: 1.7583281993865967\n",
      "where 87 loss: 1.9925349950790405\n",
      "where 88 loss: 1.8389520645141602\n",
      "where 89 loss: 1.9485135078430176\n",
      "where 90 loss: 1.8602665662765503\n",
      "where 91 loss: 1.772060751914978\n",
      "where 92 loss: 1.7958154678344727\n",
      "where 93 loss: 1.7973763942718506\n",
      "where 94 loss: 1.924813985824585\n",
      "where 95 loss: 1.9561179876327515\n",
      "where 96 loss: 1.8335250616073608\n",
      "where 97 loss: 1.8187267780303955\n",
      "where 98 loss: 1.7057987451553345\n",
      "where 99 loss: 1.7403182983398438\n",
      "where 100 loss: 1.8178930282592773\n",
      "where 101 loss: 1.8711187839508057\n",
      "where 102 loss: 1.9308831691741943\n",
      "where 103 loss: 1.8183624744415283\n",
      "where 104 loss: 1.8554414510726929\n",
      "where 105 loss: 1.8547817468643188\n",
      "where 106 loss: 1.6553479433059692\n",
      "where 107 loss: 1.7016386985778809\n",
      "where 108 loss: 1.751677393913269\n",
      "where 109 loss: 1.7298108339309692\n",
      "where 110 loss: 1.8391151428222656\n",
      "where 111 loss: 1.7595382928848267\n",
      "where 112 loss: 1.7251293659210205\n",
      "where 113 loss: 1.6854674816131592\n",
      "where 114 loss: 1.8769025802612305\n",
      "where 115 loss: 1.630260944366455\n",
      "where 116 loss: 1.5328059196472168\n",
      "where 117 loss: 1.60636568069458\n",
      "where 118 loss: 1.784042477607727\n",
      "where 119 loss: 1.8137190341949463\n",
      "where 120 loss: 1.7661726474761963\n",
      "where 121 loss: 1.5643632411956787\n",
      "where 122 loss: 1.807210087776184\n",
      "where 123 loss: 1.6678895950317383\n",
      "where 124 loss: 1.7214667797088623\n",
      "where 125 loss: 1.7717759609222412\n",
      "where 126 loss: 1.7502387762069702\n",
      "where 127 loss: 1.7635737657546997\n",
      "where 128 loss: 1.5447520017623901\n",
      "where 129 loss: 1.7904423475265503\n",
      "where 130 loss: 1.6423147916793823\n",
      "where 131 loss: 1.7090917825698853\n",
      "where 132 loss: 1.6669961214065552\n",
      "where 133 loss: 1.6342087984085083\n",
      "where 134 loss: 1.6743017435073853\n",
      "where 135 loss: 1.6681326627731323\n",
      "where 136 loss: 1.6620105504989624\n",
      "where 137 loss: 1.7954728603363037\n",
      "where 138 loss: 1.6140474081039429\n",
      "where 139 loss: 1.613476276397705\n",
      "where 140 loss: 1.685454249382019\n",
      "where 141 loss: 1.789209008216858\n",
      "where 142 loss: 1.6045242547988892\n",
      "where 143 loss: 1.862197995185852\n",
      "where 144 loss: 1.7489832639694214\n",
      "where 145 loss: 1.5716300010681152\n",
      "where 146 loss: 1.6798675060272217\n",
      "where 147 loss: 1.7021290063858032\n",
      "where 148 loss: 1.670608401298523\n",
      "where 149 loss: 1.6906689405441284\n",
      "where 150 loss: 1.760856032371521\n",
      "where 151 loss: 1.734378695487976\n",
      "where 152 loss: 1.5861669778823853\n",
      "where 153 loss: 1.6086969375610352\n",
      "where 154 loss: 1.7177897691726685\n",
      "where 155 loss: 1.8537451028823853\n",
      "where 156 loss: 1.7327263355255127\n",
      "where 157 loss: 1.6392742395401\n",
      "where 158 loss: 1.7043113708496094\n",
      "where 159 loss: 1.7722125053405762\n",
      "where 160 loss: 1.673661708831787\n",
      "where 161 loss: 1.6413793563842773\n",
      "where 162 loss: 1.6069186925888062\n",
      "where 163 loss: 1.6131199598312378\n",
      "where 164 loss: 1.6685614585876465\n",
      "where 165 loss: 1.7267154455184937\n",
      "where 166 loss: 1.6139061450958252\n",
      "where 167 loss: 1.588321328163147\n",
      "where 168 loss: 1.692383885383606\n",
      "where 169 loss: 1.4902786016464233\n",
      "where 170 loss: 1.4731171131134033\n",
      "where 171 loss: 1.6012424230575562\n",
      "where 172 loss: 1.6751925945281982\n",
      "where 173 loss: 1.5685150623321533\n",
      "where 174 loss: 1.6505250930786133\n",
      "where 175 loss: 1.5930449962615967\n",
      "where 176 loss: 1.621921420097351\n",
      "where 177 loss: 1.883872628211975\n",
      "where 178 loss: 1.5311179161071777\n",
      "where 179 loss: 1.67718505859375\n",
      "where 180 loss: 1.4251686334609985\n",
      "where 181 loss: 1.7646987438201904\n",
      "where 182 loss: 1.4804044961929321\n",
      "where 183 loss: 1.6900153160095215\n",
      "where 184 loss: 1.5922809839248657\n",
      "where 185 loss: 1.63981294631958\n",
      "where 186 loss: 1.5832066535949707\n",
      "where 187 loss: 1.5317264795303345\n",
      "where 188 loss: 1.5824726819992065\n",
      "where 189 loss: 1.6528135538101196\n",
      "where 190 loss: 1.603258490562439\n",
      "where 191 loss: 1.7058367729187012\n",
      "where 192 loss: 1.664228081703186\n",
      "where 193 loss: 1.4545893669128418\n",
      "where 194 loss: 1.4404716491699219\n",
      "where 195 loss: 1.5655016899108887\n",
      "where 196 loss: 1.6701349020004272\n",
      "where 197 loss: 1.691836953163147\n",
      "where 198 loss: 1.3654991388320923\n",
      "where 199 loss: 1.6642601490020752\n",
      "where 200 loss: 1.7395710945129395\n",
      "where 201 loss: 1.5493589639663696\n",
      "where 202 loss: 1.727073311805725\n",
      "where 203 loss: 1.4955412149429321\n",
      "where 204 loss: 1.695266842842102\n",
      "where 205 loss: 1.7708804607391357\n",
      "where 206 loss: 1.4669424295425415\n",
      "where 207 loss: 1.508352518081665\n",
      "where 208 loss: 1.5597031116485596\n",
      "where 209 loss: 1.444770097732544\n",
      "where 210 loss: 1.6019558906555176\n",
      "where 211 loss: 1.4986834526062012\n",
      "where 212 loss: 1.5409528017044067\n",
      "where 213 loss: 1.6905832290649414\n",
      "where 214 loss: 1.6303704977035522\n",
      "where 215 loss: 1.5494813919067383\n",
      "where 216 loss: 1.476680040359497\n",
      "where 217 loss: 1.629477858543396\n",
      "where 218 loss: 1.6146031618118286\n",
      "where 219 loss: 1.5486468076705933\n",
      "where 220 loss: 1.5017932653427124\n",
      "where 221 loss: 1.4145106077194214\n",
      "where 222 loss: 1.5568219423294067\n",
      "where 223 loss: 1.5333338975906372\n",
      "where 224 loss: 1.5192674398422241\n",
      "where 225 loss: 1.4347745180130005\n",
      "where 226 loss: 1.4760181903839111\n",
      "where 227 loss: 1.6672064065933228\n",
      "where 228 loss: 1.6106904745101929\n",
      "where 229 loss: 1.6156883239746094\n",
      "where 230 loss: 1.6182348728179932\n",
      "where 231 loss: 1.3383952379226685\n",
      "where 232 loss: 1.5292656421661377\n",
      "where 233 loss: 1.435083270072937\n",
      "where 234 loss: 1.5818523168563843\n",
      "where 235 loss: 1.4526488780975342\n",
      "where 236 loss: 1.3600666522979736\n",
      "where 237 loss: 1.4786509275436401\n",
      "where 238 loss: 1.4481611251831055\n",
      "where 239 loss: 1.6093887090682983\n",
      "where 240 loss: 1.3389663696289062\n",
      "where 241 loss: 1.4394909143447876\n",
      "where 242 loss: 1.6385716199874878\n",
      "where 243 loss: 1.384252667427063\n",
      "where 244 loss: 1.4459387063980103\n",
      "where 245 loss: 1.4076011180877686\n",
      "where 246 loss: 1.4461926221847534\n",
      "where 247 loss: 1.6139516830444336\n",
      "where 248 loss: 1.5265690088272095\n",
      "where 249 loss: 1.3830028772354126\n",
      "where 250 loss: 1.438513994216919\n",
      "where 251 loss: 1.4840909242630005\n",
      "where 252 loss: 1.4964573383331299\n",
      "where 253 loss: 1.518263578414917\n",
      "where 254 loss: 1.7107113599777222\n",
      "where 255 loss: 1.389296531677246\n",
      "where 256 loss: 1.49649977684021\n",
      "where 257 loss: 1.507509469985962\n",
      "where 258 loss: 1.4020901918411255\n",
      "where 259 loss: 1.508521556854248\n",
      "where 260 loss: 1.2551867961883545\n",
      "where 261 loss: 1.4324390888214111\n",
      "where 262 loss: 1.6124296188354492\n",
      "where 263 loss: 1.3697749376296997\n",
      "where 264 loss: 1.5155515670776367\n",
      "where 265 loss: 1.4492515325546265\n",
      "where 266 loss: 1.522977590560913\n",
      "where 267 loss: 1.4155651330947876\n",
      "where 268 loss: 1.563744306564331\n",
      "where 269 loss: 1.5700135231018066\n",
      "where 270 loss: 1.6498531103134155\n",
      "where 271 loss: 1.3746627569198608\n",
      "where 272 loss: 1.555281400680542\n",
      "where 273 loss: 1.4297914505004883\n",
      "where 274 loss: 1.5530105829238892\n",
      "where 275 loss: 1.343755841255188\n",
      "where 276 loss: 1.5217928886413574\n",
      "where 277 loss: 1.4144349098205566\n",
      "where 278 loss: 1.5607680082321167\n",
      "where 279 loss: 1.5576249361038208\n",
      "where 280 loss: 1.4034521579742432\n",
      "where 281 loss: 1.661118507385254\n",
      "where 282 loss: 1.4204133749008179\n",
      "where 283 loss: 1.4532541036605835\n",
      "where 284 loss: 1.4719467163085938\n",
      "where 285 loss: 1.5844806432724\n",
      "where 286 loss: 1.4613287448883057\n",
      "where 287 loss: 1.5122555494308472\n",
      "where 288 loss: 1.3467326164245605\n",
      "where 289 loss: 1.3203206062316895\n",
      "where 290 loss: 1.5653811693191528\n",
      "where 291 loss: 1.5542148351669312\n",
      "where 292 loss: 1.3635445833206177\n",
      "where 293 loss: 1.3044553995132446\n",
      "where 294 loss: 1.4430365562438965\n",
      "where 295 loss: 1.3948322534561157\n",
      "where 296 loss: 1.6067146062850952\n",
      "where 297 loss: 1.4165517091751099\n",
      "where 298 loss: 1.5815834999084473\n",
      "where 299 loss: 1.4295074939727783\n",
      "where 300 loss: 1.3880870342254639\n",
      "where 301 loss: 1.545059323310852\n",
      "where 302 loss: 1.5294793844223022\n",
      "where 303 loss: 1.3738638162612915\n",
      "where 304 loss: 1.5104384422302246\n",
      "where 305 loss: 1.4889415502548218\n",
      "where 306 loss: 1.4931095838546753\n",
      "where 307 loss: 1.6872203350067139\n",
      "where 308 loss: 1.5145519971847534\n",
      "where 309 loss: 1.4581100940704346\n",
      "where 310 loss: 1.2779258489608765\n",
      "where 311 loss: 1.3720297813415527\n",
      "where 312 loss: 1.3341069221496582\n",
      "where 313 loss: 1.4494165182113647\n",
      "where 314 loss: 1.5046278238296509\n",
      "where 315 loss: 1.3055191040039062\n",
      "where 316 loss: 1.532365083694458\n",
      "where 317 loss: 1.3957231044769287\n",
      "where 318 loss: 1.6323301792144775\n",
      "where 319 loss: 1.3615407943725586\n",
      "where 320 loss: 1.5524601936340332\n",
      "where 321 loss: 1.2995229959487915\n",
      "where 322 loss: 1.6331552267074585\n",
      "where 323 loss: 1.5317186117172241\n",
      "where 324 loss: 1.4949754476547241\n",
      "where 325 loss: 1.501571536064148\n",
      "where 326 loss: 1.6177197694778442\n",
      "where 327 loss: 1.4996505975723267\n",
      "where 328 loss: 1.7195965051651\n",
      "where 329 loss: 1.6537648439407349\n",
      "where 330 loss: 1.6217597723007202\n",
      "where 331 loss: 1.3852362632751465\n",
      "where 332 loss: 1.4386472702026367\n",
      "where 333 loss: 1.5292919874191284\n",
      "where 334 loss: 1.5400500297546387\n",
      "where 335 loss: 1.5742491483688354\n",
      "where 336 loss: 1.680738091468811\n",
      "where 337 loss: 1.4709151983261108\n",
      "where 338 loss: 1.5126515626907349\n",
      "where 339 loss: 1.5370047092437744\n",
      "where 340 loss: 1.4829679727554321\n",
      "where 341 loss: 1.2911195755004883\n",
      "where 342 loss: 1.3050285577774048\n",
      "where 343 loss: 1.4941911697387695\n",
      "where 344 loss: 1.4884521961212158\n",
      "where 345 loss: 1.3931868076324463\n",
      "where 346 loss: 1.368177890777588\n",
      "where 347 loss: 1.350947380065918\n",
      "where 348 loss: 1.4963535070419312\n",
      "where 349 loss: 1.425761103630066\n",
      "where 350 loss: 1.3170970678329468\n",
      "where 351 loss: 1.5727804899215698\n",
      "where 352 loss: 1.4220956563949585\n",
      "where 353 loss: 1.4697097539901733\n",
      "where 354 loss: 1.3557958602905273\n",
      "where 355 loss: 1.2809637784957886\n",
      "where 356 loss: 1.6055656671524048\n",
      "where 357 loss: 1.4615380764007568\n",
      "where 358 loss: 1.4391884803771973\n",
      "where 359 loss: 1.4378416538238525\n",
      "where 360 loss: 1.4447931051254272\n",
      "where 361 loss: 1.4947270154953003\n",
      "where 362 loss: 1.4385606050491333\n",
      "where 363 loss: 1.3162542581558228\n",
      "where 364 loss: 1.3512699604034424\n",
      "where 365 loss: 1.3961280584335327\n",
      "where 366 loss: 1.2671524286270142\n",
      "where 367 loss: 1.3499493598937988\n",
      "where 368 loss: 1.4764890670776367\n",
      "where 369 loss: 1.4955434799194336\n",
      "where 370 loss: 1.4612480401992798\n",
      "where 371 loss: 1.5295416116714478\n",
      "where 372 loss: 1.385124921798706\n",
      "where 373 loss: 1.4446982145309448\n",
      "where 374 loss: 1.4018073081970215\n",
      "where 375 loss: 1.544973611831665\n",
      "where 376 loss: 1.5559906959533691\n",
      "where 377 loss: 1.4654388427734375\n",
      "where 378 loss: 1.247041940689087\n",
      "where 379 loss: 1.4952915906906128\n",
      "where 380 loss: 1.4736484289169312\n",
      "where 381 loss: 1.6026103496551514\n",
      "where 382 loss: 1.5123919248580933\n",
      "where 383 loss: 1.3836573362350464\n",
      "where 384 loss: 1.4022340774536133\n",
      "where 385 loss: 1.1945136785507202\n",
      "where 386 loss: 1.5269215106964111\n",
      "where 387 loss: 1.2294024229049683\n",
      "where 388 loss: 1.2634837627410889\n",
      "where 389 loss: 1.471492886543274\n",
      "where 390 loss: 1.4022163152694702\n",
      "where 391 loss: 1.3501293659210205\n",
      "where 392 loss: 1.464218020439148\n",
      "where 393 loss: 1.3684767484664917\n",
      "where 394 loss: 1.3952845335006714\n",
      "where 395 loss: 1.337557077407837\n",
      "where 396 loss: 1.6761364936828613\n",
      "where 397 loss: 1.5028855800628662\n",
      "where 398 loss: 1.4070030450820923\n",
      "where 399 loss: 1.2869151830673218\n",
      "where 400 loss: 1.1736669540405273\n",
      "where 401 loss: 1.484674334526062\n",
      "where 402 loss: 1.4276679754257202\n",
      "where 403 loss: 1.3718621730804443\n",
      "where 404 loss: 1.553687572479248\n",
      "where 405 loss: 1.1971068382263184\n",
      "where 406 loss: 1.2560786008834839\n",
      "where 407 loss: 1.402113437652588\n",
      "where 408 loss: 1.346925973892212\n",
      "where 409 loss: 1.617118239402771\n",
      "where 410 loss: 1.4803972244262695\n",
      "where 411 loss: 1.4041651487350464\n",
      "where 412 loss: 1.4031012058258057\n",
      "where 413 loss: 1.3168578147888184\n",
      "where 414 loss: 1.3739404678344727\n",
      "where 415 loss: 1.5214084386825562\n",
      "where 416 loss: 1.2612987756729126\n",
      "where 417 loss: 1.2987127304077148\n",
      "where 418 loss: 1.3001072406768799\n",
      "where 419 loss: 1.3802915811538696\n",
      "where 420 loss: 1.44877290725708\n",
      "where 421 loss: 1.3962053060531616\n",
      "where 422 loss: 1.3392002582550049\n",
      "where 423 loss: 1.1833409070968628\n",
      "where 424 loss: 1.2181099653244019\n",
      "where 425 loss: 1.5359758138656616\n",
      "where 426 loss: 1.4365262985229492\n",
      "where 427 loss: 1.271332859992981\n",
      "where 428 loss: 1.4182534217834473\n",
      "where 429 loss: 1.5478661060333252\n",
      "where 430 loss: 1.351538062095642\n",
      "where 431 loss: 1.335984230041504\n",
      "where 432 loss: 1.6068392992019653\n",
      "where 433 loss: 1.2867765426635742\n",
      "where 434 loss: 1.495082974433899\n",
      "where 435 loss: 1.2516114711761475\n",
      "where 436 loss: 1.276403546333313\n",
      "where 437 loss: 1.4696003198623657\n",
      "where 438 loss: 1.486962080001831\n",
      "where 439 loss: 1.2658075094223022\n",
      "where 440 loss: 1.4994995594024658\n",
      "where 441 loss: 1.4294459819793701\n",
      "where 442 loss: 1.46990168094635\n",
      "where 443 loss: 1.4863038063049316\n",
      "where 444 loss: 1.459953784942627\n",
      "where 445 loss: 1.4821285009384155\n",
      "where 446 loss: 1.358296513557434\n",
      "where 447 loss: 1.4877206087112427\n",
      "where 448 loss: 1.3745688199996948\n",
      "where 449 loss: 1.5004432201385498\n",
      "where 450 loss: 1.1931402683258057\n",
      "where 451 loss: 1.3982232809066772\n",
      "where 452 loss: 1.412569522857666\n",
      "where 453 loss: 1.3196487426757812\n",
      "where 454 loss: 1.2868714332580566\n",
      "where 455 loss: 1.1182152032852173\n",
      "where 456 loss: 1.2992199659347534\n",
      "where 457 loss: 1.50765061378479\n",
      "where 458 loss: 1.2161598205566406\n",
      "where 459 loss: 1.4163784980773926\n",
      "where 460 loss: 1.3132641315460205\n",
      "where 461 loss: 1.1879671812057495\n",
      "where 462 loss: 1.296823263168335\n",
      "where 463 loss: 1.3850855827331543\n",
      "where 464 loss: 1.3503775596618652\n",
      "where 465 loss: 1.6287344694137573\n",
      "where 466 loss: 1.3464561700820923\n",
      "where 467 loss: 1.2564587593078613\n",
      "where 468 loss: 1.4381753206253052\n",
      "where 469 loss: 1.4157710075378418\n",
      "where 470 loss: 1.4099050760269165\n",
      "where 471 loss: 1.1631945371627808\n",
      "where 472 loss: 1.288458228111267\n",
      "where 473 loss: 1.3074989318847656\n",
      "where 474 loss: 1.500221848487854\n",
      "where 475 loss: 1.1556495428085327\n",
      "where 476 loss: 1.2084416151046753\n",
      "where 477 loss: 1.3964353799819946\n",
      "where 478 loss: 1.525362491607666\n",
      "where 479 loss: 1.3476197719573975\n",
      "where 480 loss: 1.7180293798446655\n",
      "where 481 loss: 1.357628583908081\n",
      "where 482 loss: 1.3789663314819336\n",
      "where 483 loss: 1.491670846939087\n",
      "where 484 loss: 1.4003472328186035\n",
      "where 485 loss: 1.4224411249160767\n",
      "where 486 loss: 1.1570075750350952\n",
      "where 487 loss: 1.4159729480743408\n",
      "where 488 loss: 1.3187835216522217\n",
      "where 489 loss: 1.3163775205612183\n",
      "where 490 loss: 1.3315403461456299\n",
      "where 491 loss: 1.2483267784118652\n",
      "where 492 loss: 1.1343891620635986\n",
      "where 493 loss: 1.3803513050079346\n",
      "where 494 loss: 1.3008530139923096\n",
      "where 495 loss: 1.2993247509002686\n",
      "where 496 loss: 1.373630404472351\n",
      "where 497 loss: 1.253435492515564\n",
      "where 498 loss: 1.167273998260498\n",
      "where 499 loss: 1.431349754333496\n",
      "epoch: 0 total_correct 20517  loss: 801.7758920192719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network =  AlexNet(num_classes=10, init_weights=True).to('cuda')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.0001)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    images = batch[0].to('cuda')\n",
    "    labels = batch[1].to('cuda')\n",
    "    \n",
    "    preds = network(images)\n",
    "    loss = F.cross_entropy(preds,labels)\n",
    "    #------------要将梯度归零 因为pytorch会累加梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds,labels)\n",
    "    print('where',count,'loss:',loss.item())\n",
    "    count +=1\n",
    "print(\"epoch:\",0,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct 20268  loss: 806.9835592508316\n",
      "epoch: 1 total_correct 28628  loss: 596.1484953761101\n",
      "epoch: 2 total_correct 32552  loss: 494.194091796875\n",
      "epoch: 3 total_correct 34893  loss: 429.7414914369583\n",
      "epoch: 4 total_correct 36687  loss: 379.80759143829346\n",
      "epoch: 5 total_correct 38038  loss: 339.41496101021767\n",
      "epoch: 6 total_correct 39211  loss: 308.43861266970634\n",
      "epoch: 7 total_correct 40138  loss: 282.90161913633347\n",
      "epoch: 8 total_correct 40925  loss: 258.51754492521286\n",
      "epoch: 9 total_correct 41716  loss: 237.63916638493538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83432"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network =  AlexNet(num_classes=10, init_weights=True).to('cuda')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.0001)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        images = batch[0].to('cuda')\n",
    "        labels = batch[1].to('cuda')\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        #------------要将梯度归零 因为pytorch会累加梯度\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    print(\"epoch:\",epoch,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct/len(train_set)\n",
    "\n",
    "#0.73242 5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct 42263  loss: 218.0058457404375\n",
      "epoch: 1 total_correct 43093  loss: 196.00184440612793\n",
      "epoch: 2 total_correct 43656  loss: 179.17380042374134\n",
      "epoch: 3 total_correct 44229  loss: 164.29390043020248\n",
      "epoch: 4 total_correct 44613  loss: 152.51626919209957\n",
      "epoch: 5 total_correct 45088  loss: 139.85669092833996\n",
      "epoch: 6 total_correct 45380  loss: 128.40011589229107\n",
      "epoch: 7 total_correct 45725  loss: 121.24254102259874\n",
      "epoch: 8 total_correct 45956  loss: 113.42267447710037\n",
      "epoch: 9 total_correct 46267  loss: 105.65239979326725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92534"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        images = batch[0].to('cuda')\n",
    "        labels = batch[1].to('cuda')\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        #------------要将梯度归零 因为pytorch会累加梯度\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    print(\"epoch:\",epoch,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct/len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = torchvision.datasets.CIFAR10(\n",
    "    root='../data/CIFAR10/'\n",
    "    ,train=False\n",
    "    ,download=False\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize((224, 224))\n",
    "        \n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 total_correct 8106  loss: 63.83726391196251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in val_loader:\n",
    "        images = batch[0].to('cuda')\n",
    "        labels = batch[1].to('cuda')\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        #------------要将梯度归零 因为pytorch会累加梯度\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    print(\"epoch:\",epoch,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct/len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct/len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(),'pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
