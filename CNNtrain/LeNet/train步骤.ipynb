{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from model import LeNet\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"using {} device.\".format(device))\n",
    "\n",
    "# data_transform = {\n",
    "#     \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                      transforms.RandomHorizontalFlip(),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "#     \"val\": transforms.Compose([transforms.Resize((224, 224)),  # cannot 224, must (224, 224)\n",
    "#                                    transforms.ToTensor(),\n",
    "#                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "\n",
    "# data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "# image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
    "# assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "# train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
    "#                                          transform=data_transform[\"train\"])\n",
    "# train_num = len(train_dataset)\n",
    "\n",
    "# # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}\n",
    "# flower_list = train_dataset.class_to_idx\n",
    "# cla_dict = dict((val, key) for key, val in flower_list.items())\n",
    "# # write dict into json file\n",
    "# json_str = json.dumps(cla_dict, indent=4)\n",
    "# with open('class_indices.json', 'w') as json_file:\n",
    "#     json_file.write(json_str)\n",
    "\n",
    "# batch_size = 32\n",
    "# nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "# print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                                batch_size=batch_size, shuffle=True,\n",
    "#                                                num_workers=nw)\n",
    "\n",
    "# validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
    "#                                             transform=data_transform[\"val\"])\n",
    "# val_num = len(validate_dataset)\n",
    "# validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "#                                                   batch_size=4, shuffle=False,\n",
    "#                                                   num_workers=nw)\n",
    "\n",
    "# print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "#                                                                            val_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b182eed6c61e47c8bce0181e26065ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10/\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='../data/CIFAR10/'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.2946665287017822\n",
      "loss2: 2.2938451766967773\n"
     ]
    }
   ],
   "source": [
    "network = LeNet()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size = 100)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.01)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images,labels = batch\n",
    "\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "\n",
    "loss.backward()#计算梯度\n",
    "optimizer.step()#更新权重\n",
    "\n",
    "#-----------------------\n",
    "print('loss1:',loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "print('loss2:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where 0\n",
      "where 1\n",
      "where 2\n",
      "where 3\n",
      "where 4\n",
      "where 5\n",
      "where 6\n",
      "where 7\n",
      "where 8\n",
      "where 9\n",
      "where 10\n",
      "where 11\n",
      "where 12\n",
      "where 13\n",
      "where 14\n",
      "where 15\n",
      "where 16\n",
      "where 17\n",
      "where 18\n",
      "where 19\n",
      "where 20\n",
      "where 21\n",
      "where 22\n",
      "where 23\n",
      "where 24\n",
      "where 25\n",
      "where 26\n",
      "where 27\n",
      "where 28\n",
      "where 29\n",
      "where 30\n",
      "where 31\n",
      "where 32\n",
      "where 33\n",
      "where 34\n",
      "where 35\n",
      "where 36\n",
      "where 37\n",
      "where 38\n",
      "where 39\n",
      "where 40\n",
      "where 41\n",
      "where 42\n",
      "where 43\n",
      "where 44\n",
      "where 45\n",
      "where 46\n",
      "where 47\n",
      "where 48\n",
      "where 49\n",
      "where 50\n",
      "where 51\n",
      "where 52\n",
      "where 53\n",
      "where 54\n",
      "where 55\n",
      "where 56\n",
      "where 57\n",
      "where 58\n",
      "where 59\n",
      "where 60\n",
      "where 61\n",
      "where 62\n",
      "where 63\n",
      "where 64\n",
      "where 65\n",
      "where 66\n",
      "where 67\n",
      "where 68\n",
      "where 69\n",
      "where 70\n",
      "where 71\n",
      "where 72\n",
      "where 73\n",
      "where 74\n",
      "where 75\n",
      "where 76\n",
      "where 77\n",
      "where 78\n",
      "where 79\n",
      "where 80\n",
      "where 81\n",
      "where 82\n",
      "where 83\n",
      "where 84\n",
      "where 85\n",
      "where 86\n",
      "where 87\n",
      "where 88\n",
      "where 89\n",
      "where 90\n",
      "where 91\n",
      "where 92\n",
      "where 93\n",
      "where 94\n",
      "where 95\n",
      "where 96\n",
      "where 97\n",
      "where 98\n",
      "where 99\n",
      "where 100\n",
      "where 101\n",
      "where 102\n",
      "where 103\n",
      "where 104\n",
      "where 105\n",
      "where 106\n",
      "where 107\n",
      "where 108\n",
      "where 109\n",
      "where 110\n",
      "where 111\n",
      "where 112\n",
      "where 113\n",
      "where 114\n",
      "where 115\n",
      "where 116\n",
      "where 117\n",
      "where 118\n",
      "where 119\n",
      "where 120\n",
      "where 121\n",
      "where 122\n",
      "where 123\n",
      "where 124\n",
      "where 125\n",
      "where 126\n",
      "where 127\n",
      "where 128\n",
      "where 129\n",
      "where 130\n",
      "where 131\n",
      "where 132\n",
      "where 133\n",
      "where 134\n",
      "where 135\n",
      "where 136\n",
      "where 137\n",
      "where 138\n",
      "where 139\n",
      "where 140\n",
      "where 141\n",
      "where 142\n",
      "where 143\n",
      "where 144\n",
      "where 145\n",
      "where 146\n",
      "where 147\n",
      "where 148\n",
      "where 149\n",
      "where 150\n",
      "where 151\n",
      "where 152\n",
      "where 153\n",
      "where 154\n",
      "where 155\n",
      "where 156\n",
      "where 157\n",
      "where 158\n",
      "where 159\n",
      "where 160\n",
      "where 161\n",
      "where 162\n",
      "where 163\n",
      "where 164\n",
      "where 165\n",
      "where 166\n",
      "where 167\n",
      "where 168\n",
      "where 169\n",
      "where 170\n",
      "where 171\n",
      "where 172\n",
      "where 173\n",
      "where 174\n",
      "where 175\n",
      "where 176\n",
      "where 177\n",
      "where 178\n",
      "where 179\n",
      "where 180\n",
      "where 181\n",
      "where 182\n",
      "where 183\n",
      "where 184\n",
      "where 185\n",
      "where 186\n",
      "where 187\n",
      "where 188\n",
      "where 189\n",
      "where 190\n",
      "where 191\n",
      "where 192\n",
      "where 193\n",
      "where 194\n",
      "where 195\n",
      "where 196\n",
      "where 197\n",
      "where 198\n",
      "where 199\n",
      "where 200\n",
      "where 201\n",
      "where 202\n",
      "where 203\n",
      "where 204\n",
      "where 205\n",
      "where 206\n",
      "where 207\n",
      "where 208\n",
      "where 209\n",
      "where 210\n",
      "where 211\n",
      "where 212\n",
      "where 213\n",
      "where 214\n",
      "where 215\n",
      "where 216\n",
      "where 217\n",
      "where 218\n",
      "where 219\n",
      "where 220\n",
      "where 221\n",
      "where 222\n",
      "where 223\n",
      "where 224\n",
      "where 225\n",
      "where 226\n",
      "where 227\n",
      "where 228\n",
      "where 229\n",
      "where 230\n",
      "where 231\n",
      "where 232\n",
      "where 233\n",
      "where 234\n",
      "where 235\n",
      "where 236\n",
      "where 237\n",
      "where 238\n",
      "where 239\n",
      "where 240\n",
      "where 241\n",
      "where 242\n",
      "where 243\n",
      "where 244\n",
      "where 245\n",
      "where 246\n",
      "where 247\n",
      "where 248\n",
      "where 249\n",
      "where 250\n",
      "where 251\n",
      "where 252\n",
      "where 253\n",
      "where 254\n",
      "where 255\n",
      "where 256\n",
      "where 257\n",
      "where 258\n",
      "where 259\n",
      "where 260\n",
      "where 261\n",
      "where 262\n",
      "where 263\n",
      "where 264\n",
      "where 265\n",
      "where 266\n",
      "where 267\n",
      "where 268\n",
      "where 269\n",
      "where 270\n",
      "where 271\n",
      "where 272\n",
      "where 273\n",
      "where 274\n",
      "where 275\n",
      "where 276\n",
      "where 277\n",
      "where 278\n",
      "where 279\n",
      "where 280\n",
      "where 281\n",
      "where 282\n",
      "where 283\n",
      "where 284\n",
      "where 285\n",
      "where 286\n",
      "where 287\n",
      "where 288\n",
      "where 289\n",
      "where 290\n",
      "where 291\n",
      "where 292\n",
      "where 293\n",
      "where 294\n",
      "where 295\n",
      "where 296\n",
      "where 297\n",
      "where 298\n",
      "where 299\n",
      "where 300\n",
      "where 301\n",
      "where 302\n",
      "where 303\n",
      "where 304\n",
      "where 305\n",
      "where 306\n",
      "where 307\n",
      "where 308\n",
      "where 309\n",
      "where 310\n",
      "where 311\n",
      "where 312\n",
      "where 313\n",
      "where 314\n",
      "where 315\n",
      "where 316\n",
      "where 317\n",
      "where 318\n",
      "where 319\n",
      "where 320\n",
      "where 321\n",
      "where 322\n",
      "where 323\n",
      "where 324\n",
      "where 325\n",
      "where 326\n",
      "where 327\n",
      "where 328\n",
      "where 329\n",
      "where 330\n",
      "where 331\n",
      "where 332\n",
      "where 333\n",
      "where 334\n",
      "where 335\n",
      "where 336\n",
      "where 337\n",
      "where 338\n",
      "where 339\n",
      "where 340\n",
      "where 341\n",
      "where 342\n",
      "where 343\n",
      "where 344\n",
      "where 345\n",
      "where 346\n",
      "where 347\n",
      "where 348\n",
      "where 349\n",
      "where 350\n",
      "where 351\n",
      "where 352\n",
      "where 353\n",
      "where 354\n",
      "where 355\n",
      "where 356\n",
      "where 357\n",
      "where 358\n",
      "where 359\n",
      "where 360\n",
      "where 361\n",
      "where 362\n",
      "where 363\n",
      "where 364\n",
      "where 365\n",
      "where 366\n",
      "where 367\n",
      "where 368\n",
      "where 369\n",
      "where 370\n",
      "where 371\n",
      "where 372\n",
      "where 373\n",
      "where 374\n",
      "where 375\n",
      "where 376\n",
      "where 377\n",
      "where 378\n",
      "where 379\n",
      "where 380\n",
      "where 381\n",
      "where 382\n",
      "where 383\n",
      "where 384\n",
      "where 385\n",
      "where 386\n",
      "where 387\n",
      "where 388\n",
      "where 389\n",
      "where 390\n",
      "where 391\n",
      "where 392\n",
      "where 393\n",
      "where 394\n",
      "where 395\n",
      "where 396\n",
      "where 397\n",
      "where 398\n",
      "where 399\n",
      "where 400\n",
      "where 401\n",
      "where 402\n",
      "where 403\n",
      "where 404\n",
      "where 405\n",
      "where 406\n",
      "where 407\n",
      "where 408\n",
      "where 409\n",
      "where 410\n",
      "where 411\n",
      "where 412\n",
      "where 413\n",
      "where 414\n",
      "where 415\n",
      "where 416\n",
      "where 417\n",
      "where 418\n",
      "where 419\n",
      "where 420\n",
      "where 421\n",
      "where 422\n",
      "where 423\n",
      "where 424\n",
      "where 425\n",
      "where 426\n",
      "where 427\n",
      "where 428\n",
      "where 429\n",
      "where 430\n",
      "where 431\n",
      "where 432\n",
      "where 433\n",
      "where 434\n",
      "where 435\n",
      "where 436\n",
      "where 437\n",
      "where 438\n",
      "where 439\n",
      "where 440\n",
      "where 441\n",
      "where 442\n",
      "where 443\n",
      "where 444\n",
      "where 445\n",
      "where 446\n",
      "where 447\n",
      "where 448\n",
      "where 449\n",
      "where 450\n",
      "where 451\n",
      "where 452\n",
      "where 453\n",
      "where 454\n",
      "where 455\n",
      "where 456\n",
      "where 457\n",
      "where 458\n",
      "where 459\n",
      "where 460\n",
      "where 461\n",
      "where 462\n",
      "where 463\n",
      "where 464\n",
      "where 465\n",
      "where 466\n",
      "where 467\n",
      "where 468\n",
      "where 469\n",
      "where 470\n",
      "where 471\n",
      "where 472\n",
      "where 473\n",
      "where 474\n",
      "where 475\n",
      "where 476\n",
      "where 477\n",
      "where 478\n",
      "where 479\n",
      "where 480\n",
      "where 481\n",
      "where 482\n",
      "where 483\n",
      "where 484\n",
      "where 485\n",
      "where 486\n",
      "where 487\n",
      "where 488\n",
      "where 489\n",
      "where 490\n",
      "where 491\n",
      "where 492\n",
      "where 493\n",
      "where 494\n",
      "where 495\n",
      "where 496\n",
      "where 497\n",
      "where 498\n",
      "where 499\n",
      "epoch: 0 total_correct 20712  loss: 802.4066696166992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41424"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = LeNet()#.to('cuda')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.001)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    images = batch[0]#.to('cuda')\n",
    "    labels = batch[1]#.to('cuda')\n",
    "    \n",
    "    preds = network(images)\n",
    "    loss = F.cross_entropy(preds,labels)\n",
    "    #------------要将梯度归零 因为pytorch会累加梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds,labels)\n",
    "    print('where',count)\n",
    "    count +=1\n",
    "print(\"epoch:\",0,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct 22206  loss: 2119.9413304924965\n",
      "epoch: 1 total_correct 29206  loss: 1613.7031079530716\n",
      "epoch: 2 total_correct 32064  loss: 1404.07358700037\n",
      "epoch: 3 total_correct 33825  loss: 1272.2525480687618\n",
      "epoch: 4 total_correct 35109  loss: 1168.6952145695686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70218"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = LeNet()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=36)\n",
    "optimizer = optim.Adam(network.parameters(),lr = 0.001)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        images,labels = batch\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        #------------要将梯度归零 因为pytorch会累加梯度\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    print(\"epoch:\",epoch,\"total_correct\",total_correct,\" loss:\",total_loss)\n",
    "total_correct/len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(),'pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
