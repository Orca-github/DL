{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "### 卷积计算\n",
    "#### 卷积为正方形\n",
    "# n * n input  \n",
    "# f * f filter  \n",
    "# padding p stride s  \n",
    "# 输出大小O为：  \n",
    "#         O = (n-f+2p)/s  + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self,in_planes):\n",
    "        super(ChannelAttention,self).__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes,in_channels // 16,kernel_size=1,bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // 16,in_planes,kernel_size=1,biase=False)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        avg_out = self.avg_pool(x)\n",
    "        avg_out = self.fc1(avg_out)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)\n",
    "        \n",
    "        max_out = self.max_pool(x)\n",
    "        max_out = self.fc1(max_out)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)\n",
    "\n",
    "        out = avg_out + max_out\n",
    "        out = self.sigmod(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,in_planes):\n",
    "        super(SpatialAttention,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2,1,kernel_size=7,paddin=3,biase=False)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        avg_out = torch.mean(x,dim=1,keepdim=True)\n",
    "        \n",
    "        max_out = torch.max(x,dim=1,keepdim=True)\n",
    "\n",
    "        out = torch.cat([avg_out,max_out],dim=1)\n",
    "        out = self.conv1(out)\n",
    "        out = self.sigmod(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18layer 34layer 结构\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1#用于标注是否需要转换卷积核个数\n",
    "    def __init__(self,in_channel,out_channel,stride =1,downsample=None):\n",
    "        #downsample 下采样 在高层网络中的 进行降维（宽高，非通道）的那个操作 56，56，64--*stride：2---1，1，128*----28，28，128\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=out_channel,\n",
    "                                kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        #若 stride=1 paddi=1则 宽高不会改变 output= (input -3 +2*1)/1 + 1 = input\n",
    "        #若 stride=2 padding=1则： output = (input -3 + 2*1) /2 +1\n",
    "        # = input/2 +0.5\n",
    "        # = input/2(向下取整)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel,out_channels=out_channel,\n",
    "                                kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.channel = ChannelAttention(self.expansion*out_channel)\n",
    "        self.spatial = SpatialAttention()\n",
    "    \n",
    "    def forward(self,t):\n",
    "        identity = t\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(t)\n",
    "        t = self.conv1(t)\n",
    "        t = self.bn1(t)\n",
    "        t = self.relu(t)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = self.bn2(t)\n",
    "        \n",
    "        t += identity\n",
    "        t = self.relu(t)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数inplace：是否选择进行覆盖运算，默认值为False。\n",
    "\n",
    "# 若为True则进行覆盖运算，例如：x = x +1,即对原值进行操作，又将结果直接赋值给原值。\n",
    "\n",
    "# 若为False则不进行直接覆盖运算，例如：y = x +1,x = y，即找一个中间按变量y作为传递\n",
    "\n",
    "# 具体到上述代码，inplace=True时,就对self.conv计算得到的结果直接做修改，这样做的好处就是可以节省内存，还可以节省反复申请和释放内存的时间，且最后计算结果与inplace=False相同，只要不出现错误就可以使用。\n",
    "\n",
    "#50layer 101layer 152layer 结构\n",
    "class Bottlenect(nn.Module):\n",
    "    expansion = 4#用于标注是否需要转换卷积核个数\n",
    "    def __init__(self,in_channel,out_channel,stride =1,downsample=None):\n",
    "        #downsample 下采样 在高层网络中的 进行降维（宽高，非通道）的那个操作 56，56，64--*stride：2---1，1，128*----28，28，128\n",
    "        super(Bottlenect,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=out_channel,\n",
    "                                kernel_size=1,stride=1,padding=1,bias=False)\n",
    "        #若 stride=1 paddi=1则 宽高不会改变 output= (input -3 +2*1)/1 + 1 = input\n",
    "        #若 stride=2 padding=1则： output = (input -3 + 2*1) /2 +1\n",
    "        # = input/2 +0.5\n",
    "        # = input/2(向下取整)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel,out_channels=out_channel,\n",
    "                                kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        #------------------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel,out_channels=out_channel*self.expansion,\n",
    "                                kernel_size=1,stride=1,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(implace=True)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self,t):\n",
    "        identity = t\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(t)\n",
    "        t = self.conv1(t)\n",
    "        t = self.bn1(t)\n",
    "        t = self.relu(t)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = self.bn2(t)\n",
    "        t = self.relu(t)\n",
    "\n",
    "        t = self.conv3(t)\n",
    "        t = self.bn3(t)\n",
    "        \n",
    "        t += identity\n",
    "        t = self.relu(t)\n",
    "\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM_ResNet(nn.Module):\n",
    "    def __init__(self,block,blocks_num,num_classes =1000,include_top=True):\n",
    "        super(CBAM_ResNet,self).__init__()\n",
    "        self.include_top = include_top#是否包含全连接层 用于后期搭建更为复杂的网络\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,self.in_channel,kernel_size=7,stride=2,\n",
    "                                padding=3,bias=False)#nn.conv2d中第一个参数就是输入channel 第二个参数为输出的channel\n",
    "        #输入channel3 表示 rgb彩色图像\n",
    "        #O = (n-f+2p)/s  + 1      in-224 out-112  (224-7+2p)2 + 1 =112\n",
    "        #p = 2 或 3  带回去算 向下取整 所以用 3 \n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        #pool后宽高为56*56\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        #block就是 basicblock（不需要转换为度 18 layer &34 layer 或者 bottlenect（需要转换维度）\n",
    "        self.layer1 = self._make_layer(block,64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block,128, blocks_num[1],stride = 2)\n",
    "        self.layer3 = self._make_layer(block,256, blocks_num[2],stride = 2)\n",
    "        self.layer4 = self._make_layer(block,512, blocks_num[3],stride =2)\n",
    "\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode= 'fan_out', nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def _make_layer(self,block, channel, block_num,stride=1):\n",
    "        downsample = None\n",
    "\n",
    "        #18 和 34层会跳过下 if语句   self.in_channel =64 若是需要改变维度启用bottleneck则bottle中的expansion为4 \n",
    "        #layer2 中没有传入参数 stride stirde默认为1\n",
    "        if stride!=1 or self.in_channel != channel*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel,channel * block.expansion, kernel_size=1,\n",
    "                stride = stride, bias= False),\n",
    "                nn.BatchNorm2d(channel * block.expansion)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel,channel,downsample=downsample,stride=stride))\n",
    "        self.in_channel = channel*block.expansion# 50 101 152的每个block的第三层的channel数量为1 2 层的4倍\n",
    "\n",
    "        for _ in range(1,block_num):\n",
    "            layers.append(block(self.in_channel,channel))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,t):\n",
    "        t = self.conv1(t)\n",
    "        t = self.bn1(t)\n",
    "        t = self.relu(t)\n",
    "        t = self.maxpool(t)\n",
    "\n",
    "        t = self.layer1(t)\n",
    "        t = self.layer2(t)\n",
    "        t = self.layer3(t)\n",
    "        t = self.layer4(t)\n",
    "\n",
    "        if self.include_top:\n",
    "            t = self.avgpool(t)\n",
    "            t = torch.flatten(t,1)\n",
    "            t = self.fc(t)\n",
    "        \n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18 (num_classes=1000,include_top = True):\n",
    "        return ResNet(BasicBlock,[2,2,2,2],num_classes=num_classes,include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet34 (num_classes=1000,include_top = True):\n",
    "        return ResNet(BasicBlock,[3,4,6,3],num_classes=num_classes,include_top=include_top)\n",
    "\n",
    "def resnet50 (num_classes=1000,include_top = True):\n",
    "        return ResNet(Bottlenect,[3,4,6,3],num_classes=num_classes,include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet101 (num_classes=1000,include_top = True):\n",
    "        return ResNet(Bottlenect,[3,4,23,3],num_classes=num_classes,include_top=include_top)\n",
    "\n",
    "\n",
    "#import torchvision.models.resnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
